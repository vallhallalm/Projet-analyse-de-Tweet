{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connexion au serveur sur lequel la base est hébergée.\n",
    "!pip install pymongo \n",
    "import pymongo\n",
    "import urllib\n",
    "from pprint import pprint\n",
    "db_uri = \"mongodb://admin_if29:passwordIF29%23@13.38.0.254:27017/?authMechanism=DEFAULT\"\n",
    "client = pymongo.MongoClient(db_uri)\n",
    "print(client.list_database_names())\n",
    "base_db=client.small_tweets_database\n",
    "collec_co=base_db.small_tweets_grouped_by_user\n",
    "\n",
    "pprint(collec_co.find_one({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pymongo\n",
    "import json\n",
    "import matplotlib.pyplot as plt     #for plotting data and cerating different charts\n",
    "import numpy as np                  #for math and array\n",
    "import pandas as pd                 #data for the data\n",
    "import seaborn as sns               #for plotting\n",
    "\n",
    "#TODO GET DATA FROM MONOGODB\n",
    "db_uri = \"mongodb://admin_if29:passwordIF29%23@13.38.0.254:27017/?authMechanism=DEFAULT\"\n",
    "client = pymongo.MongoClient(db_uri)\n",
    "print(client.list_database_names())\n",
    "base_db=client.small_tweets_database\n",
    "collec_co=base_db.small_tweets_final\n",
    "datas=pd.DataFrame(list(collec_co.find()))\n",
    "print('dataset shape: ',datas.shape)\n",
    "print('Summary information on the dataset')\n",
    "datas.info()\n",
    "\n",
    "#CLEANING THE DATA\n",
    "print(\"display NA values in each colums\")\n",
    "datas.isna().sum(axis=0)\n",
    "print(\"display NULL values in each colums\")\n",
    "datas.isnull().sum()\n",
    "\n",
    "print(\"remove line with NA\")\n",
    "datas= datas.dropna()\n",
    "\n",
    "datas= datas.sample(frac=1)     #Shuffle the datas to not be sorted\n",
    "\n",
    "#SPLIT THE DTATA INTO TRAIN / TEST SPLITS\n",
    "#Split the datas into 60% train and 40% test\n",
    "train_dataset, temp_test_dataset = train_test_split(datas, test_size=0.4)\n",
    "print(train_dataset.shape)\n",
    "print(temp_test_dataset.shape)\n",
    "\n",
    "#Split the test_dataset to 50% test and validation\n",
    "test_dataset, valid_dataset = train_test_split(temp_test_dataset, test_size=0.5)\n",
    "print(test_dataset.shape)\n",
    "print(valid_dataset.shape)\n",
    "\n",
    "#Resume the split's dataset\n",
    "print(f\" Train dataset       : {train_dataset.shape}\")\n",
    "print(f\" Test dataset       : {test_dataset.shape}\")\n",
    "print(f\" Validation dataset : {valid_dataset.shape}\")\n",
    "\n",
    "# Plot the relationship between each two variables to spot anything incorrect.\n",
    "train_stats = train_dataset.describe()\n",
    "train_stats.pop(\"target\")\n",
    "sns.pairplot(train_stats[train_stats.columns], diag_kind=\"kde\") # or diag_kind\n",
    "\n",
    "# Statistics on the train dataset to make sure it is in a good shape.\n",
    "# (Can display the same stat for test and validate)\n",
    "train_stats = train_dataset.describe()\n",
    "train_stats.pop(\"target\")\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats\n",
    "\n",
    "train_labels = train_dataset.pop('target')\n",
    "test_labels = test_dataset.pop('target')\n",
    "valid_labels = valid_dataset.pop('target')\n",
    "\n",
    "#DATA NORMALISATION / SCALING\n",
    "#Subtract the mean of the training data and divide\n",
    "# by the standard deviation of the training data.\n",
    "\n",
    "# define a function to normalize the data set.\n",
    "def norm(x):\n",
    "    return (x - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "normed_train_data = norm(train_dataset)\n",
    "normed_test_data = norm(test_dataset)\n",
    "normed_valid_dataset = norm(valid_dataset)\n",
    "\n",
    "# show a sample of the data after normalized\n",
    "normed_train_data.head(10)\n",
    "\n",
    "#TRAIN THE MODEL\n",
    "#Create a svm Classifier\n",
    "model = svm.SVC(C = 1, # reg paramater\n",
    "                kernel='linear', #kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’\n",
    ") # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "model.fit(normed_train_data, train_labels)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model.predict(normed_test_data)\n",
    "\n",
    "example_batch = normed_test_data[:10]\n",
    "example_result = model.predict(example_batch)\n",
    "print('predicted values: ')\n",
    "example_result\n",
    "\n",
    "#See how the training went\n",
    "from sklearn import metrics\n",
    "y_pred = model.predict(normed_train_data)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(train_labels, y_pred))\n",
    "\n",
    "y_pred = model.predict(normed_valid_dataset)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(valid_labels, y_pred))\n",
    "\n",
    "y_pred = model.predict(normed_test_data)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels, y_pred))\n",
    "\n",
    "ax= plt.subplot()\n",
    "predict_results = model.predict(normed_test_data)\n",
    "\n",
    "cm = confusion_matrix(predict_results, predict_results)\n",
    "\n",
    "sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n",
    "ax.set_title('Confusion Matrix');\n",
    "# ax.xaxis.set_ticklabels(['Positive', 'Negative']); ax.yaxis.set_ticklabels(['Positive', 'Negative']);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': 4395, 'follow_popularity': 0.009699118579999617, 'group_popularity': 0.06366260563196037, 'hashtags_freq': 3.0, 'mentions_freq': 1.0, 'verified_badge': 0, 'url_freq': 0.0, 'date_of_creation_account': 0.6751799726410301, 'tweets_frequency': 2.6789952743273476e-09}\n",
      " {'_id': 5199, 'follow_popularity': 0.001950878695273138, 'group_popularity': 0.027034805131380433, 'hashtags_freq': 2.0, 'mentions_freq': 1.0, 'verified_badge': 0, 'url_freq': 1.0, 'date_of_creation_account': 0.6786576873765858, 'tweets_frequency': 2.690058895154129e-09}\n",
      " {'_id': 5549, 'follow_popularity': 0.06468463460627301, 'group_popularity': 0.25988296545649575, 'hashtags_freq': 1.6666666666666667, 'mentions_freq': 0.3333333333333333, 'verified_badge': 1, 'url_freq': 1.0, 'date_of_creation_account': 0.6797058467485647, 'tweets_frequency': 8.08009548712096e-09}\n",
      " ...\n",
      " {'_id': 1007264625707175936, 'follow_popularity': 0.0, 'group_popularity': 0.0, 'hashtags_freq': 1.0, 'mentions_freq': 0.0, 'verified_badge': 0, 'url_freq': 0.0, 'date_of_creation_account': -2.95067239976961, 'tweets_frequency': 0.052631578947368425}\n",
      " {'_id': 1007264805630062592, 'follow_popularity': 0.0, 'group_popularity': 0.0, 'hashtags_freq': 3.0, 'mentions_freq': 0.0, 'verified_badge': 0, 'url_freq': 1.0, 'date_of_creation_account': -2.9506705745719186, 'tweets_frequency': 0.04}\n",
      " {'_id': 1007265312767709184, 'follow_popularity': 0.0, 'group_popularity': 0.0, 'hashtags_freq': 1.0, 'mentions_freq': 0.0, 'verified_badge': 0, 'url_freq': 0.0, 'date_of_creation_account': -2.950665438562625, 'tweets_frequency': 0.02857142857142857}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11240/2283614630.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollec_co\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtweets_prep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\louis\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\louis\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mscale\u001b[1;34m(X, axis, with_mean, with_std, copy)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \"\"\"  # noqa\n\u001b[1;32m--> 161\u001b[1;33m     X = check_array(X, accept_sparse='csc', copy=copy, ensure_2d=False,\n\u001b[0m\u001b[0;32m    162\u001b[0m                     \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'the scale function'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                     force_all_finite='allow-nan')\n",
      "\u001b[1;32mc:\\Users\\louis\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\louis\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    671\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32mc:\\Users\\louis\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'dict'"
     ]
    }
   ],
   "source": [
    "from sklearn import cluster\n",
    "import pymongo\n",
    "import json\n",
    "import matplotlib.pyplot as plt     \n",
    "import numpy as np                  \n",
    "import pandas as pd                 \n",
    "from sklearn import preprocessing\n",
    "\n",
    "db_uri = \"mongodb://admin_if29:passwordIF29%23@13.38.0.254:27017/?authMechanism=DEFAULT\"\n",
    "client = pymongo.MongoClient(db_uri)\n",
    "#print(client.list_database_names())\n",
    "base_db=client.small_tweets_database\n",
    "collec_co=base_db.small_tweets_final\n",
    "tweets=np.array(list(collec_co.find()))\n",
    "print(tweets.view())\n",
    "tweets_prep=preprocessing.scale(tweets)\n",
    "\n",
    "kmeans = cluster.KMeans(n_clusters=2)\n",
    "kmeans.fit(tweets_prep)\n",
    "print(kmeans.inertia_)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee2a8bf78ed58fe7d4a6da4c01dc73c50fe79a52336feed85a35ce76368bd76"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
